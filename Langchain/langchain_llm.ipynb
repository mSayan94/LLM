{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "model_name = os.getenv(\"HUGGINGFACE_MODEL_NAME\")\n",
    "huggingface_llm_model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=api_token, repo_id=model_name, model_kwargs={\n",
    "        \"max_length\": 128, \n",
    "        \"temperature\": 0.5}\n",
    ")\n",
    "print(huggingface_llm_model.predict(\"Suggest me some amazing places to visit in Delhi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=['country'],template=\"Tell me the capital of {country}\")\n",
    "prompt_template.format(country = \"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=azure_openai_model, prompt=prompt_template)\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So, the Final code block looks like this***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "model_name = os.getenv(\"HUGGINGFACE_MODEL_NAME\")\n",
    "\n",
    "# llm_model = HuggingFaceHub(\n",
    "#     huggingfacehub_api_token = api_token, repo_id=model_name, model_kwargs={\"max_length\": 128, \"temperature\": 0.5}\n",
    "# )\n",
    "\n",
    "azure_openai_model = AzureChatOpenAI(\n",
    "    azure_deployment =\"dp-gpt35-16k\",\n",
    "    azure_endpoint =os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=['country'],template=\"Tell me the capital of {country}\")\n",
    "chain = LLMChain(llm=azure_openai_model, prompt=prompt_template)\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains using Simple sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "capital_template = PromptTemplate(input_variables=['country'], template=\"Please tell me the capital of {country}\")\n",
    "capital_chain = LLMChain(llm=azure_openai_model, prompt=capital_template)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'], template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "famous_chain = LLMChain(llm=azure_openai_model, prompt=famous_template)\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[capital_chain,famous_chain])\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here's a step-by-step approach for creating a Simple Sequential LLM Chain without relying on the Hugging Face library:\n",
    "\n",
    "   1. **Import Required Modules:**\n",
    "      - Import the necessary modules, including classes like `PromptTemplate`, `LLMChain`, and `SimpleSequentialChain`.\n",
    "\n",
    "      ```python\n",
    "      from your_custom_module.prompts import PromptTemplate\n",
    "      from your_custom_module.chains import LLMChain, SimpleSequentialChain\n",
    "      ```\n",
    "\n",
    "   2. **Set Up Language Model:**\n",
    "      - Set up your language model, whether it's a custom implementation or another library.\n",
    "      - Configure any necessary parameters for the language model.\n",
    "\n",
    "      ```python\n",
    "      from your_custom_module.models import YourLanguageModel\n",
    "\n",
    "      llm_model = YourLanguageModel(model_args={\"max_length\": 128, \"temperature\": 0.5})\n",
    "      ```\n",
    "\n",
    "   3. **Create Prompt Templates:**\n",
    "      - Define prompt templates for input variables and desired prompts.\n",
    "\n",
    "      ```python\n",
    "      first_template = PromptTemplate(input_variables=['var1'], template=\"Your Custom template {var1}\")\n",
    "      second_template = PromptTemplate(input_variables=['var2'], template=\"Your Custom template {var2}\")\n",
    "      ```\n",
    "\n",
    "   4. **Create LLM Chains:**\n",
    "      - Instantiate LLM Chains using the language model and prompt templates.\n",
    "\n",
    "      ```python\n",
    "      first_chain = LLMChain(llm=llm_model, prompt=first_template)\n",
    "      second_chain = LLMChain(llm=llm_model, prompt=second_template)\n",
    "      ```\n",
    "\n",
    "   5. **Create Simple Sequential Chain:**\n",
    "      - Combine individual chains into a Simple Sequential Chain.\n",
    "\n",
    "      ```python\n",
    "      chain = SimpleSequentialChain(chains=[first_chain, second_chain])\n",
    "      ```\n",
    "\n",
    "   6. **Run the Chain:**\n",
    "      - Execute the chain with a specific input.\n",
    "\n",
    "      ```python\n",
    "      result = chain.run(\"YourInputValue\")\n",
    "      ```\n",
    "\n",
    "      The `run` method initiates the sequential execution, starting with the first chain and passing the result to the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So, the Final code block looks like this***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "model_name = os.getenv(\"HUGGINGFACE_MODEL_NAME\")\n",
    "\n",
    "# llm_model = HuggingFaceHub(\n",
    "#     huggingfacehub_api_token = api_token, repo_id=model_name, model_kwargs={\"max_length\": 128, \"temperature\": 0.5}\n",
    "# )\n",
    "\n",
    "azure_openai_model = AzureChatOpenAI(\n",
    "    azure_deployment =\"gpt-35-turbo-16k\",\n",
    "    azure_endpoint =os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "capital_template = PromptTemplate(input_variables=['country'], template=\"Please tell me the capital of {country}\")\n",
    "capital_chain = LLMChain(llm=azure_openai_model, prompt=capital_template)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'], template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "famous_chain = LLMChain(llm=azure_openai_model, prompt=famous_template)\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[capital_chain,famous_chain])\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "# model_name = os.getenv(\"HUGGINGFACE_MODEL_NAME\")\n",
    "\n",
    "# llm_model = HuggingFaceHub(\n",
    "#     huggingfacehub_api_token = api_token, repo_id=model_name, model_kwargs={\"max_length\": 128, \"temperature\": 0.5}\n",
    "# )\n",
    "\n",
    "azure_openai_model = AzureChatOpenAI(\n",
    "    azure_deployment =\"gpt-35-turbo-16k\",\n",
    "    azure_endpoint =os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "capital_template = PromptTemplate(input_variables=['country'], template=\"Please tell me the capital of {country}\")\n",
    "capital_chain = LLMChain(llm=azure_openai_model, prompt=capital_template, output_key=\"capital\")\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'], template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "famous_chain = LLMChain(llm=azure_openai_model, prompt=famous_template,output_key=\"places\")\n",
    "\n",
    "chain = SequentialChain(chains=[capital_chain,famous_chain], input_variables=['country'], output_variables=['capital',\"places\"])\n",
    "chain({'country':\"India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModels with Chat OpenAI\n",
    "\n",
    "There are three schemas with respect to ChatOpenAI:\n",
    "\n",
    "- **Human message**: This is the input that the user gives to the chatbot.\n",
    "- **System message**: This is the default message that the chatbot opens with. It is related to the domain of the chatbot.\n",
    "- **AI message**: This is the output that the chatbot gives to the user. It is generated by the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "azure_chat_llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "azure_chat_llm([\n",
    "    SystemMessage(content=\"You are a Comedian AI assistant\"),\n",
    "    HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is designed to interact with an AI model, specifically Azure's ChatGPT model (referred to as AzureChatOpenAI in the code), to generate synonyms for a given word. Let's break down the components of this code to understand how it works:  \n",
    "   \n",
    "### Import Statements  \n",
    "- `os`: A module that provides a way to use operating system dependent functionality. Here, it's used to access environment variables.  \n",
    "- `dotenv`: From the `python-dotenv` package, this is used to load environment variables from a `.env` file into the Python script. This is useful for hiding sensitive information like API keys.  \n",
    "- `langchain.chat_models`, `langchain.prompts.chat`, and `langchain.schema`: These are imports from the `langchain` library, which provides tools for working with language models and constructing chat-based applications.  \n",
    "   \n",
    "### Environment Variables  \n",
    "- `load_dotenv()`: This function call loads the environment variables from a `.env` file into the script, allowing the script to access them using `os.getenv()`.  \n",
    "   \n",
    "### CommaSeparatedParser Class  \n",
    "- A custom parser class that inherits from `BaseOutputParser`. Its purpose is to take a string output from the AI model and split it into a list by commas. This is used to parse the generated synonyms.  \n",
    "   \n",
    "### AzureChatOpenAI Initialization  \n",
    "- An instance of `AzureChatOpenAI` is created with deployment name, endpoint, and API key obtained from environment variables. This object is configured to interact with Azure's AI model.  \n",
    "   \n",
    "### Chat Prompt Templates  \n",
    "- `Systemtemplate` and `HumanTemplate` are defined to structure the conversation with the AI. The system template sets the context for the AI, telling it to generate synonyms in a comma-separated format. The human template is a placeholder for the input word.  \n",
    "- `ChatPrompt` uses these templates to create a structured input for the AI model, where the system's instructions and the human's query are clearly defined.  \n",
    "   \n",
    "### Chaining and Invocation  \n",
    "- `chain` represents a sequence of operations: it takes an input (`ChatPrompt`), processes it through the AzureChatOpenAI model (`azure_chat_llm`), and then parses the output using `CommaSeperatedParser`.  \n",
    "- `chain.invoke({\"text\":\"intelligent\"})` triggers the entire process with \"intelligent\" as the input word. It should result in the AI generating synonyms for \"intelligent\", formatting them in a comma-separated string, and then parsing that string into a list of synonyms.  \n",
    "   \n",
    "In summary, this code is set up to ask an Azure AI model for synonyms of a given word (\"intelligent\" in this example), expecting the synonyms to be returned in a comma-separated format. The process involves setting up a conversational prompt, sending this prompt to the AI, and parsing the AI's response to extract the synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explain the below Code\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Creating a class of CommaSeperatedParser\n",
    "class CommaSeperatedParser(BaseOutputParser):\n",
    "    def parse(self, output:str):\n",
    "        return output.strip().split(\",\")\n",
    "    \n",
    "# Initialising Azure Chat OpenAI\n",
    "azure_chat_llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"OPENAI_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "Systemtemplate = \"You are a helpful AI assistant. Whenever you are given with a word, you have to generate 5 synonyms of that word in comma seperated format\"\n",
    "HumanTemplate = \"{text}\"\n",
    "ChatPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", Systemtemplate),\n",
    "        (\"human\", HumanTemplate)\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = ChatPrompt|azure_chat_llm|CommaSeperatedParser()\n",
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
